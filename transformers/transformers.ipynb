{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 639777918144 acquired on C:\\Users\\chenh\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to C:\\Users\\chenh\\.cache\\torch\\transformers\\tmpzi7ul2ld\n",
      "INFO:filelock:Lock 639777918144 released on C:\\Users\\chenh\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIProgress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6533b2fb8252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Load pre-trained model tokenizer (vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Tokenize input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \"\"\"\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m                         \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m                         \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m                         \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m                     )\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m                 \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storing %s in cache at %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0minitial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Downloading\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         self.container = self.status_printer(\n\u001b[1;32m--> 206\u001b[1;33m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# #187 #451 #558\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             raise ImportError(\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[1;34m\"FloatProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package transformers:\n",
      "\n",
      "NAME\n",
      "    transformers\n",
      "\n",
      "DESCRIPTION\n",
      "    # flake8: noqa\n",
      "    # There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n",
      "    # module, but to preserve other warnings. So, don't check this module at all.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    activations\n",
      "    another_try\n",
      "    benchmark_utils\n",
      "    commands (package)\n",
      "    configuration_albert\n",
      "    configuration_auto\n",
      "    configuration_bart\n",
      "    configuration_bert\n",
      "    configuration_camembert\n",
      "    configuration_ctrl\n",
      "    configuration_distilbert\n",
      "    configuration_electra\n",
      "    configuration_flaubert\n",
      "    configuration_gpt2\n",
      "    configuration_mmbt\n",
      "    configuration_openai\n",
      "    configuration_roberta\n",
      "    configuration_t5\n",
      "    configuration_transfo_xl\n",
      "    configuration_utils\n",
      "    configuration_xlm\n",
      "    configuration_xlm_roberta\n",
      "    configuration_xlnet\n",
      "    convert_albert_original_tf_checkpoint_to_pytorch\n",
      "    convert_bart_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_bert_original_tf_checkpoint_to_pytorch\n",
      "    convert_bert_pytorch_checkpoint_to_original_tf\n",
      "    convert_dialogpt_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_electra_original_tf_checkpoint_to_pytorch\n",
      "    convert_gpt2_original_tf_checkpoint_to_pytorch\n",
      "    convert_openai_original_tf_checkpoint_to_pytorch\n",
      "    convert_pytorch_checkpoint_to_tf2\n",
      "    convert_roberta_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_t5_original_tf_checkpoint_to_pytorch\n",
      "    convert_transfo_xl_original_tf_checkpoint_to_pytorch\n",
      "    convert_xlm_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_xlnet_original_tf_checkpoint_to_pytorch\n",
      "    data (package)\n",
      "    file\n",
      "    file_utils\n",
      "    filep\n",
      "    hf_api\n",
      "    modelcard\n",
      "    modeling_albert\n",
      "    modeling_auto\n",
      "    modeling_bart\n",
      "    modeling_beam_search\n",
      "    modeling_bert\n",
      "    modeling_camembert\n",
      "    modeling_ctrl\n",
      "    modeling_distilbert\n",
      "    modeling_electra\n",
      "    modeling_encoder_decoder\n",
      "    modeling_flaubert\n",
      "    modeling_gpt2\n",
      "    modeling_mmbt\n",
      "    modeling_openai\n",
      "    modeling_roberta\n",
      "    modeling_t5\n",
      "    modeling_tf_albert\n",
      "    modeling_tf_auto\n",
      "    modeling_tf_bert\n",
      "    modeling_tf_camembert\n",
      "    modeling_tf_ctrl\n",
      "    modeling_tf_distilbert\n",
      "    modeling_tf_electra\n",
      "    modeling_tf_flaubert\n",
      "    modeling_tf_gpt2\n",
      "    modeling_tf_openai\n",
      "    modeling_tf_pytorch_utils\n",
      "    modeling_tf_roberta\n",
      "    modeling_tf_t5\n",
      "    modeling_tf_transfo_xl\n",
      "    modeling_tf_transfo_xl_utilities\n",
      "    modeling_tf_utils\n",
      "    modeling_tf_xlm\n",
      "    modeling_tf_xlm_roberta\n",
      "    modeling_tf_xlnet\n",
      "    modeling_transfo_xl\n",
      "    modeling_transfo_xl_utilities\n",
      "    modeling_utils\n",
      "    modeling_xlm\n",
      "    modeling_xlm_roberta\n",
      "    modeling_xlnet\n",
      "    optimization\n",
      "    optimization_tf\n",
      "    pipelines\n",
      "    tokenization_albert\n",
      "    tokenization_auto\n",
      "    tokenization_bart\n",
      "    tokenization_bert\n",
      "    tokenization_bert_japanese\n",
      "    tokenization_camembert\n",
      "    tokenization_ctrl\n",
      "    tokenization_distilbert\n",
      "    tokenization_electra\n",
      "    tokenization_flaubert\n",
      "    tokenization_gpt2\n",
      "    tokenization_openai\n",
      "    tokenization_roberta\n",
      "    tokenization_t5\n",
      "    tokenization_transfo_xl\n",
      "    tokenization_utils\n",
      "    tokenization_xlm\n",
      "    tokenization_xlm_roberta\n",
      "    tokenization_xlnet\n",
      "    try\n",
      "    utils_encoder_decoder\n",
      "\n",
      "DATA\n",
      "    ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'albert-base-v1': 'https://s3....\n",
      "    ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.a...\n",
      "    ALL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.ama...\n",
      "    ALL_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.amaz...\n",
      "    BART_PRETRAINED_MODEL_ARCHIVE_MAP = {'bart-large': 'https://s3.amazona...\n",
      "    BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'bert-base-cased': 'https://s3.a...\n",
      "    BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'bert-base-cased': 'https://s3.am...\n",
      "    CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'camembert-base': 'https://...\n",
      "    CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'camembert-base': 'https://s...\n",
      "    CONFIG_MAPPING = OrderedDict([('t5', <class 'transformers.configu...sf...\n",
      "    CONFIG_NAME = 'config.json'\n",
      "    CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'ctrl': 'https://storage.googlea...\n",
      "    CTRL_PRETRAINED_MODEL_ARCHIVE_MAP = {'ctrl': 'https://storage.googleap...\n",
      "    DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilbert-base-cased': '...\n",
      "    DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilbert-base-cased': 'h...\n",
      "    ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP = {'google/electra-base-discrimi...\n",
      "    ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP = {'google/electra-base-discrimin...\n",
      "    FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'flaubert-base-cased': 'http...\n",
      "    FLAUBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'flaubert-base-cased': 'https...\n",
      "    GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amazon...\n",
      "    GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amazona...\n",
      "    MODEL_CARD_NAME = 'modelcard.json'\n",
      "    MODEL_FOR_PRETRAINING_MAPPING = OrderedDict([(<class 'transformers.con...\n",
      "    MODEL_FOR_QUESTION_ANSWERING_MAPPING = OrderedDict([(<class 'transform...\n",
      "    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = OrderedDict([(<class 'tran...\n",
      "    MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = OrderedDict([(<class 'transfo...\n",
      "    MODEL_MAPPING = OrderedDict([(<class 'transformers.configuration... 't...\n",
      "    MODEL_WITH_LM_HEAD_MAPPING = OrderedDict([(<class 'transformers.config...\n",
      "    OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'openai-gpt': 'https://s3....\n",
      "    OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {'openai-gpt': 'https://s3.a...\n",
      "    PYTORCH_PRETRAINED_BERT_CACHE = WindowsPath('C:/Users/chenh/.cache/tor...\n",
      "    PYTORCH_TRANSFORMERS_CACHE = WindowsPath('C:/Users/chenh/.cache/torch/...\n",
      "    ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilroberta-base': 'https:...\n",
      "    ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilroberta-base': 'https:/...\n",
      "    SPIECE_UNDERLINE = 'â–'\n",
      "    T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws.co...\n",
      "    T5_PRETRAINED_MODEL_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws.com...\n",
      "    TF2_WEIGHTS_NAME = 'tf_model.h5'\n",
      "    TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s...\n",
      "    TF_ALL_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.a...\n",
      "    TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'bert-base-cased': 'https://s3...\n",
      "    TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
      "    TF_CTRL_PRETRAINED_MODEL_ARCHIVE_MAP = {'ctrl': 'https://s3.amazonaws....\n",
      "    TF_DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilbert-base-cased':...\n",
      "    TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP = {'google/electra-base-discri...\n",
      "    TF_FLAUBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
      "    TF_GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amaz...\n",
      "    TF_MODEL_FOR_PRETRAINING_MAPPING = OrderedDict([(<class 'transformers....\n",
      "    TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING = OrderedDict([(<class 'transf...\n",
      "    TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = OrderedDict([(<class 't...\n",
      "    TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = OrderedDict([(<class 'tran...\n",
      "    TF_MODEL_MAPPING = OrderedDict([(<class 'transformers.configuration......\n",
      "    TF_MODEL_WITH_LM_HEAD_MAPPING = OrderedDict([(<class 'transformers.con...\n",
      "    TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {'openai-gpt': 'https://s...\n",
      "    TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilroberta-base': 'http...\n",
      "    TF_T5_PRETRAINED_MODEL_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws....\n",
      "    TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {'transfo-xl-wt103': 'htt...\n",
      "    TF_WEIGHTS_NAME = 'model.ckpt'\n",
      "    TF_XLM_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s...\n",
      "    TF_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
      "    TF_XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlnet-base-cased': 'https://...\n",
      "    TOKENIZER_MAPPING = OrderedDict([(<class 'transformers.configuration.....\n",
      "    TRANSFORMERS_CACHE = WindowsPath('C:/Users/chenh/.cache/torch/transfor...\n",
      "    TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'transfo-xl-wt103': 'https...\n",
      "    TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {'transfo-xl-wt103': 'https:...\n",
      "    WEIGHTS_NAME = 'pytorch_model.bin'\n",
      "    XLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s3....\n",
      "    XLM_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s3.a...\n",
      "    XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlm-roberta-base': 'http...\n",
      "    XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-roberta-base': 'https...\n",
      "    XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlnet-base-cased': 'https://s3...\n",
      "    XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlnet-base-cased': 'https://s3....\n",
      "    glue_output_modes = {'cola': 'classification', 'mnli': 'classification...\n",
      "    glue_processors = {'cola': <class 'transformers.data.processors.glue.C...\n",
      "    glue_tasks_num_labels = {'cola': 2, 'mnli': 3, 'mrpc': 2, 'qnli': 2, '...\n",
      "    logger = <Logger transformers (INFO)>\n",
      "    xnli_output_modes = {'xnli': 'classification'}\n",
      "    xnli_processors = {'xnli': <class 'transformers.data.processors.xnli.X...\n",
      "    xnli_tasks_num_labels = {'xnli': 3}\n",
      "\n",
      "VERSION\n",
      "    2.8.0\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "help(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
