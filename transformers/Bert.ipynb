{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\zhangyongxing\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "from transformers import  BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level= logging.INFO)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', \n",
    "                             'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\zhangyongxing\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\ZHANGY~1\\AppData\\Local\\Temp\\tmpa3kkv_kh\n",
      "\n",
      "  0%|                                                                                     | 0/440473133 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|                                                                     | 1024/440473133 [00:00<80:31:18, 1519.51B/s]\u001b[A\n",
      "  0%|                                                                    | 26624/440473133 [00:01<56:50:32, 2152.38B/s]\u001b[A\n",
      "  0%|                                                                    | 44032/440473133 [00:01<40:34:58, 3014.60B/s]\u001b[A\n",
      "  0%|                                                                    | 60416/440473133 [00:01<29:04:10, 4208.40B/s]\u001b[A\n",
      "  0%|                                                                    | 64512/440473133 [00:02<30:05:42, 4064.96B/s]\u001b[A\n",
      "  0%|                                                                    | 78848/440473133 [00:04<26:45:54, 4570.54B/s]\u001b[A\n",
      "  0%|                                                                    | 96256/440473133 [00:05<20:08:43, 6072.23B/s]\u001b[A\n",
      "  0%|                                                                   | 113664/440473133 [00:10<24:29:42, 4993.74B/s]\u001b[A\n",
      "  0%|                                                                   | 131072/440473133 [00:11<19:15:42, 6350.24B/s]\u001b[A\n",
      "  0%|                                                                   | 148480/440473133 [00:12<14:53:51, 8210.20B/s]\u001b[A\n",
      "  0%|                                                                   | 165888/440473133 [00:13<12:33:00, 9745.42B/s]\u001b[A\n",
      "  0%|                                                                   | 183296/440473133 [00:13<9:29:23, 12887.67B/s]\u001b[A\n",
      "  0%|                                                                   | 200704/440473133 [00:14<8:03:28, 15177.51B/s]\u001b[A\n",
      "  0%|                                                                   | 218112/440473133 [00:14<6:20:54, 19263.74B/s]\u001b[A\n",
      "  0%|                                                                   | 235520/440473133 [00:15<6:33:49, 18631.02B/s]\u001b[A\n",
      "  0%|                                                                   | 252928/440473133 [00:17<8:17:56, 14734.42B/s]\u001b[A\n",
      "  0%|                                                                   | 270336/440473133 [00:18<7:18:57, 16714.18B/s]\u001b[A\n",
      "  0%|                                                                   | 285696/440473133 [00:28<7:18:56, 16714.18B/s]\u001b[A\n",
      "  0%|                                                                   | 286720/440473133 [00:35<43:38:52, 2801.36B/s]\u001b[A\n",
      "  0%|                                                                   | 286720/440473133 [00:48<43:38:52, 2801.36B/s]\u001b[A"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin' to download pretrained weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSysCallError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1839\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1662\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1663\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1664\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Unexpected EOF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSysCallError\u001b[0m: (10054, 'WSAECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m                 if (\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: (10054, 'WSAECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    750\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    540\u001b[0m                         \u001b[1;31m# Content-Length are caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Connection broken: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: OSError(\"(10054, \\'WSAECONNRESET\\')\",)', OSError(\"(10054, 'WSAECONNRESET')\",))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                 \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: OSError(\"(10054, \\'WSAECONNRESET\\')\",)', OSError(\"(10054, 'WSAECONNRESET')\",))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0b344539db24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load pre-trained model (weights)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Set the model in evaluation mode to deactivate the DropOut modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# This is IMPORTANT to have reproducible results during evaluation!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\BCRJ\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                             \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                             [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME])\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin' to download pretrained weights."
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BertTokenizer in module transformers.tokenization_bert:\n",
      "\n",
      "class BertTokenizer(transformers.tokenization_utils.PreTrainedTokenizer)\n",
      " |  Constructs a BertTokenizer.\n",
      " |  :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
      " |      do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n",
      " |      do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
      " |      max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
      " |          minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
      " |      never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
      " |          do_wordpiece_only=False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertTokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, **kwargs)\n",
      " |      Constructs a BertTokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
      " |          **do_lower_case**: (`optional`) boolean (default True)\n",
      " |              Whether to lower case the input\n",
      " |              Only has an effect when do_basic_tokenize=True\n",
      " |          **do_basic_tokenize**: (`optional`) boolean (default True)\n",
      " |              Whether to do basic tokenization before wordpiece.\n",
      " |          **never_split**: (`optional`) list of string\n",
      " |              List of tokens which will never be split during tokenization.\n",
      " |              Only has an effect when do_basic_tokenize=True\n",
      " |          **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
      " |              Whether to tokenize Chinese characters.\n",
      " |              This should likely be deactivated for Japanese:\n",
      " |              see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
      " |      by concatenating and adding special tokens.\n",
      " |      A BERT sequence has the following format:\n",
      " |          single sequence: [CLS] X [SEP]\n",
      " |          pair of sequences: [CLS] A [SEP] B [SEP]\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None)\n",
      " |      Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
      " |      A BERT sequence pair mask has the following format:\n",
      " |      0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " |      | first sequence    | second sequence\n",
      " |      \n",
      " |      if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False)\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0: list of ids (must not contain special tokens)\n",
      " |          token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
      " |              for sequence pairs\n",
      " |          already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
      " |              special tokens for the model\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
      " |  \n",
      " |  save_vocabulary(self, vocab_path)\n",
      " |      Save the tokenizer vocabulary to a directory or file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  vocab_size\n",
      " |      Size of the base vocabulary (without the added tokens)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'bert-base-cased': 512, 'bert-base-cased-fine...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'bert-base-cased': {'do_lower_case': ...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'vocab_file': {'bert-base-cased': 'https...\n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': 'vocab.txt'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict)\n",
      " |      Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n",
      " |      to class attributes. If special tokens are NOT in the vocabulary, they are added\n",
      " |      to it (indexed starting from the last index of the current vocabulary).\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - special tokens are carefully handled by the tokenizer (they are never split)\n",
      " |      - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n",
      " |              [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens)\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n",
      " |      vocabulary, they are added to it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids, skip_special_tokens=False)\n",
      " |      Converts a single index or a sequence of indices (integers) in a token \"\n",
      " |      (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens)\n",
      " |      Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n",
      " |      (resp. a sequence of ids), using the vocabulary.\n",
      " |  \n",
      " |  decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
      " |      Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
      " |      with options to remove special tokens and clean up tokenization spaces.\n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n",
      " |          skip_special_tokens: if set to True, will replace special tokens.\n",
      " |          clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n",
      " |  \n",
      " |  encode(self, text, text_pair=None, add_special_tokens=False, max_length=None, stride=0, truncation_strategy='longest_first', return_tensors=None, **kwargs)\n",
      " |      Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method)\n",
      " |          text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
      " |              `convert_tokens_to_ids` method)\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      " |              If there are overflowing tokens, those will be added to the returned dictionary\n",
      " |          stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |  \n",
      " |  encode_plus(self, text, text_pair=None, add_special_tokens=False, max_length=None, stride=0, truncation_strategy='longest_first', return_tensors=None, **kwargs)\n",
      " |      Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n",
      " |      the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method)\n",
      " |          text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
      " |              `convert_tokens_to_ids` method)\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      " |              If there are overflowing tokens, those will be added to the returned dictionary\n",
      " |          stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |  \n",
      " |  num_added_tokens(self, pair=False)\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      Note:\n",
      " |          This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n",
      " |          inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n",
      " |              number of added tokens in the case of a single sequence if set to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to sequences\n",
      " |  \n",
      " |  prepare_for_model(self, ids, pair_ids=None, max_length=None, add_special_tokens=False, stride=0, truncation_strategy='longest_first', return_tensors=None)\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
      " |      It adds special tokens, truncates\n",
      " |      sequences if overflowing while taking into account the special tokens and manages a window stride for\n",
      " |      overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: list of tokenized input ids. Can be obtained from a string by chaining the\n",
      " |              `tokenize` and `convert_tokens_to_ids` methods.\n",
      " |          pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n",
      " |              `tokenize` and `convert_tokens_to_ids` methods.\n",
      " |          max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n",
      " |              list of inputs.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |      \n",
      " |      Return:\n",
      " |          A Dictionary of shape::\n",
      " |      \n",
      " |              {\n",
      " |                  input_ids: list[int],\n",
      " |                  overflowing_tokens: list[int] if a ``max_length`` is specified, else None\n",
      " |                  special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True``\n",
      " |              }\n",
      " |      \n",
      " |          With the fields:\n",
      " |              ``input_ids``: list of tokens to be fed to a model\n",
      " |      \n",
      " |              ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
      " |      \n",
      " |              ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
      " |              tokens and 1 specifying sequence tokens.\n",
      " |  \n",
      " |  save_pretrained(self, save_directory)\n",
      " |      Save the tokenizer vocabulary files together with:\n",
      " |          - added tokens,\n",
      " |          - special-tokens-to-class-attributes-mapping,\n",
      " |          - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n",
      " |      \n",
      " |      This won't save modifications other than (added tokens and special token mapping) you may have\n",
      " |      applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
      " |  \n",
      " |  tokenize(self, text, **kwargs)\n",
      " |      Converts a string in a sequence of tokens (string), using the tokenizer.\n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based\n",
      " |      vocabularies (BPE/SentencePieces/WordPieces).\n",
      " |      \n",
      " |      Take care of added tokens.\n",
      " |  \n",
      " |  truncate_sequences(self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy='longest_first', stride=0)\n",
      " |      Truncates a sequence pair in place to the maximum length.\n",
      " |      truncation_strategy: string selected in the following options:\n",
      " |          - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |              starting from the longest one at each token (when there is a pair of input sequences).\n",
      " |              Overflowing tokens only contains overflow from the first sequence.\n",
      " |          - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n",
      " |          - 'only_second': Only truncate the second sequence\n",
      " |          - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  from_pretrained(*inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path: either:\n",
      " |      \n",
      " |              - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
      " |              - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
      " |              - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
      " |      \n",
      " |          cache_dir: (`optional`) string:\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
      " |      \n",
      " |          force_download: (`optional`) boolean, default False:\n",
      " |              Force to (re-)download the vocabulary files and override the cached versions if they exists.\n",
      " |      \n",
      " |          proxies: (`optional`) dict, default None:\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
      " |              The proxies are used on each request.\n",
      " |      \n",
      " |          inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
      " |      \n",
      " |          kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
      " |      \n",
      " |          # Download vocabulary from S3 and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string)\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set.\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n",
      " |      class attributes (cls_token, unk_token...).\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n",
      " |      (cls_token, unk_token...).\n",
      " |  \n",
      " |  bos_token\n",
      " |      Beginning of sentence token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      End of sentence token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      Id of the end of sentence token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      Padding token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      Id of the padding token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n",
      " |      values ('<unk>', '<cls>'...)\n",
      " |  \n",
      " |  unk_token\n",
      " |      Unknown token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      Id of the unknown token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BertTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
